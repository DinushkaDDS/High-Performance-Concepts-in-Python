{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Cluster and Queue Processing in Python**\r\n",
    "\r\n",
    "\r\n",
    "Cluster can be simply identified as a set of computers which are prepared to do a common task. By using a cluster we can access very large amount of resources like high number of CPU, RAM and storage space.\r\n",
    "\r\n",
    "One of the major benefits of clustering is that it allows us to scale resources based on our requirement.\r\n",
    "Also its ability to add more machines give an additional advantage of reliability.\r\n",
    "But on the other hand clusters have high cost to maintain. Also programming for clusters require additional effort as well. \r\n",
    "\r\n",
    "> BTW read about `Chaos Monkey` package from Netflix, which randomly kill your program processes to check resillency. Very Cool!\r\n",
    "> Also read about `Circus` and `supervisord` python packages which helps in reliably start cluster components.\r\n",
    "\r\n",
    "In python there are few tools available for clustering. Below include some of them explained."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## IPython Parrallel\r\n",
    "\r\n",
    "This package have interfaces for local and remote processing engines where data and jobs can be shared among the remote machines. (jupyter notebooks also use this interface.) To properly configure a IPython cluster require configurations changes (for security, MPI etc.) therefore need experience before deploying a cluster into a critical environment.\r\n",
    "\r\n",
    "<center><image src=\"./img/26.png\" width=\"250\"/></center>\r\n",
    "\r\n",
    "This project consists with 4 major components mentioned below.\r\n",
    "\r\n",
    "* Engine: This is an extension of the IPython kernal. (it is synchronized interpreter.) When there are multiple engines available distributed/parrallel compting possible.\r\n",
    "* Controller: Controller processes provide an interface for working with a set of engines. It is responsible for work distribution and load balanced work scheduler.\r\n",
    "* Hub: This is the central component of the IPython. It keeps track of engine connections, schedulers, clients, as well as all task requests and results.\r\n",
    "* Schedulers: Every action performed on the engine goes through a schedular. It hides the synchronous nature and provides asynchronous interface to the users.\r\n",
    "\r\n",
    "Other than that we need the Client, for connecting to a cluster. Also For each execution model, there is a corresponding View. These views allow users/Clients to interact with a set of engines through the interface.\r\n",
    "\r\n",
    "Below is simple usage of IPython parrallel in a local machine.\r\n",
    "\r\n",
    "You can read the documentation [here](https://ipyparallel.readthedocs.io/en/latest/tutorial/index.html) for more details."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import ipyparallel as ipp\r\n",
    "cluster = ipp.Cluster(n=4)\r\n",
    "await cluster.start_cluster()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting 4 engines with <class 'ipyparallel.cluster.launcher.LocalEngineSetLauncher'>\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<Cluster(cluster_id='1655642958-max5', profile='default', controller=<running>, engine_sets=['1655642959'])>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "rc = cluster.connect_client_sync()\r\n",
    "rc.wait_for_engines(n=4)\r\n",
    "rc.ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "rc[:].apply_sync(lambda: (\"Hello, World\"))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Hello, World', 'Hello, World', 'Hello, World', 'Hello, World']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "cluster.stop_cluster_sync()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stopping controller\n",
      "Stopping engine(s): 1655642959\n",
      "Controller stopped: {'exit_code': 1, 'pid': 19472, 'identifier': 'ipcontroller-1655642958-max5-17084'}\n",
      "engine set stopped 1655642959: {'engines': {'0': {'exit_code': 1, 'pid': 8268, 'identifier': '0'}, '1': {'exit_code': 1, 'pid': 19620, 'identifier': '1'}, '2': {'exit_code': 1, 'pid': 6384, 'identifier': '2'}, '3': {'exit_code': 1, 'pid': 8732, 'identifier': '3'}}, 'exit_code': 1}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seemingly Ipython parrallel module is very interesting and easy to use. We can execute the functions we created earlier without much of trouble and get results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import time\r\n",
    "import ipyparallel as ipp\r\n",
    "from ipyparallel import require\r\n",
    "import random\r\n",
    "\r\n",
    "@require('random')\r\n",
    "def calc_point_inside_circle(num_of_estimates):\r\n",
    "    \r\n",
    "    print(f\"Executing calc_point_inside_circle with {num_of_estimates:,}\")\r\n",
    "\r\n",
    "    trials_inside_circle = 0\r\n",
    "\r\n",
    "    for step in range(int(num_of_estimates)):\r\n",
    "        x = random.uniform(0,1)\r\n",
    "        y = random.uniform(0,1)\r\n",
    "\r\n",
    "        is_inside_circle = 1 if (x**2 + y**2) <= 1 else 0\r\n",
    "        trials_inside_circle += is_inside_circle\r\n",
    "\r\n",
    "    return trials_inside_circle\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    total_trials = 1e8\r\n",
    "    num_workers = 4\r\n",
    "    trials_per_worker = total_trials/num_workers\r\n",
    "    trials_per_processes = [trials_per_worker]*num_workers\r\n",
    "\r\n",
    "    with ipp.Cluster() as rc:\r\n",
    "\r\n",
    "        print(f'Cluster Initialized with {len(rc.ids)} engines')\r\n",
    "        view = rc.load_balanced_view()\r\n",
    "\r\n",
    "        # submit the tasks\r\n",
    "        start_time = time.time()\r\n",
    "        asyncresult = view.map_async(calc_point_inside_circle, trials_per_processes)\r\n",
    "        asyncresult.wait_interactive()\r\n",
    "        result = asyncresult.get()\r\n",
    "\r\n",
    "        print(f\"Estimates of {result} made with {len(result)} engines.\")\r\n",
    "\r\n",
    "        pi_estimate = sum(result) * 4 / total_trials\r\n",
    "        print(\"Estimated pi\", pi_estimate)\r\n",
    "        print(f\"Time consumed: {time.time()-start_time}\")\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting 12 engines with <class 'ipyparallel.cluster.launcher.LocalEngineSetLauncher'>\n",
      "100%|██████████| 12/12 [00:05<00:00,  2.12engine/s]\n",
      "Cluster Initialized with 12 engines\n",
      "calc_point_inside_circle: 100%|██████████| 4/4 [00:22<00:00,  5.61s/tasks]\n",
      "Estimates of [19633519, 19631933, 19634736, 19634490] made with 4 engines.\n",
      "Estimated pi 3.14138712\n",
      "Time consumed: 22.4538357257843\n",
      "Stopping engine(s): 1655644321\n",
      "engine set stopped 1655644321: {'engines': {'0': {'exit_code': 1, 'pid': 17644, 'identifier': '0'}, '1': {'exit_code': 1, 'pid': 2692, 'identifier': '1'}, '2': {'exit_code': 1, 'pid': 17504, 'identifier': '2'}, '3': {'exit_code': 1, 'pid': 18236, 'identifier': '3'}, '4': {'exit_code': 1, 'pid': 6688, 'identifier': '4'}, '5': {'exit_code': 1, 'pid': 6120, 'identifier': '5'}, '6': {'exit_code': 1, 'pid': 23264, 'identifier': '6'}, '7': {'exit_code': 1, 'pid': 16880, 'identifier': '7'}, '8': {'exit_code': 1, 'pid': 19876, 'identifier': '8'}, '9': {'exit_code': 1, 'pid': 19096, 'identifier': '9'}, '10': {'exit_code': 1, 'pid': 10284, 'identifier': '10'}, '11': {'exit_code': 1, 'pid': 23132, 'identifier': '11'}}, 'exit_code': 1}\n",
      "Stopping controller\n",
      "Controller stopped: {'exit_code': 1, 'pid': 4468, 'identifier': 'ipcontroller-1655644320-ux2j-17084'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assuming the cluster initialized properly in multiple remote machines, execution is super easy. (though we dont share any data between processes and does not communicate. XD)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pandas and Dask\r\n",
    "\r\n",
    "`Dask` can be conidered as the Spark lite version. If you dont need replicated writes, multimachine fail reliability and dont want to support storage environment like hadoop Dask is the way to go for processing larger than RAM datasets.\r\n",
    "\r\n",
    "In dask, just like spark build up a task graph before executing the actual transformation/process (lazy evaluation). I am not going to test that here. But it is worth noting the package.\r\n",
    "\r\n",
    "\r\n",
    "### Swifter\r\n",
    "\r\n",
    "Swifter is a package which built on top of dask to provide additional parrallalization to your data. It Tries to vectorize your dataframe. If it seems fine it will apply otherwise execute using dask.\r\n",
    "\r\n",
    "> Theres a new library named `Vaex` which provides pandas like interface to handle larger than RAM, datasets. Apparently it is specially good at manipulating string, so will worth a shot!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NSQ library for queues and pub/sub\r\n",
    "\r\n",
    "NSQ is a highly performant distributed and robust messaging platform. It provides REST API which can be called using any language and therefore language agnostic. The importance about NSQ is that it provides fundaental gurantees about the message delivery. It provides 2 simple well know design patterns for that namely queues and pub/subs.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('High_Performance_Concepts_in_Python-45__Ubn2': virtualenv)"
  },
  "interpreter": {
   "hash": "7e345707fd4776d029d2a8a839d91a2c00b4b8e78ea36281d84f354477ba90d2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}