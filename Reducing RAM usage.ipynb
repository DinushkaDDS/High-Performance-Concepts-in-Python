{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Reducing RAM usage of Programs**\r\n",
    "\r\n",
    "All our programs consume some amount of memory. But if this memory consumption is larger than our computer memory (i.e. RAM), the program will not be able to execute. Also in general more memory usage means higher level of overhead. Therefore we should always try to reduce the memory consumption of our programs as much as possible. We can do that by several ways like using better data structures, data compression techniques, avoiding unnecessary data movements etc.\r\n",
    "\r\n",
    "For an example lets consider a list with same object in all indices vs a list with unique items in all indices and their memory cost.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext memory_profiler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%memit [0] * int(1e8)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 849.66 MiB, increment: 763.10 MiB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%memit [n for n in range(int(1e8))]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 4026.67 MiB, increment: 3940.16 MiB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that list with unique items consume close to 4GB of memory. But if we use a different data stucture to store the same amount of data memory consumption is different."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import array\r\n",
    "%memit array.array('l', range(int(1e8)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 712.28 MiB, increment: 624.44 MiB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see it only consume ~600MB of memory. This result emphasize the importance of using proper data structure for right purpose. You may think that then usage of arrays would be great for anything. But it is not. It have limited set of supported data types. Also the moment we derefence our data from array, python will build a new object which will cost memory. So if your program is going to deference data frequently and process no memory saving will occur.\r\n",
    "\r\n",
    "If we need array with more data type support, we can use numpy arrays. They act very simialar to the normal python arrays and provides better data type support.\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import numpy as np\r\n",
    "%memit arr=np.ones(int(1e8), np.int8)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 289.00 MiB, increment: 95.37 MiB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NumExpr to optimize memory\r\n",
    "\r\n",
    "During some of the numpy operations, program can create intermediate objects which may cause unexpected/unwanted memory consumption. The `NumExpr` is a tool that can both speed up and reduce the effect of such intermediate items problem. \r\n",
    "\r\n",
    "> NumExpr breaks the long vectors into shorter, cache-friendly chunks and processes each in series, so local chunks of results are calculated in a cache-friendly way.\r\n",
    "\r\n",
    "It should be noted that NumExpr support both numpy and pandas libraries and specially pandas dataframe.eval function uses NumExpr for processing if the package is available in the context.\r\n",
    "\r\n",
    "### Python's way of data storing\r\n",
    "\r\n",
    "In python, most of the objects are dynamic. Which means they take more space than more staticly typed programs. For example python python interger takes one byte. But if the interger is large that size may differ. Check below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import sys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "sys.getsizeof(0)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "sys.getsizeof(1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "sys.getsizeof(2**30-1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "sys.getsizeof(2**30)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that depending on the size of the interger memory it consume change. Same goes for lists and strings as well. But above used `sys.getsizeof` function may not reflect actual memory usage of the full object. For example it will not consider object hierarchies or underline data structures etc. So relying solely on the output given by above function can be confusing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "sys.getsizeof([b\"asdfghjklqwertyuiop\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "sys.getsizeof([b\"a\", b\"b\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Among the most common data types strings are one of the hardest type to store/process efficiently. But we can use some special data structures to compress the representation while still allowing fast operations.\r\n",
    "\r\n",
    "## **Storing Strings in RAM**\r\n",
    "\r\n",
    "Lets assume we have a huge amount of tokens to store and we have to search for a given token in our token pile. \r\n",
    "\r\n",
    "The easiest method would be storing the token pile in a list and when a query comes we can compare each and see. Obviously this is dumb. Putting aside the memory clearly this takes huge amount of time. As a somewhat improvement we can store tokens in a sorted manner. This will help us in future search operations. But may fire back at us if we are to add more tokens later.\r\n",
    "\r\n",
    "Another solution is using a set. This is indeed easy (at least in python). But memory usage is still considerable. Using a set may become problematic is you need do get number of tokens available (count) like operations. In such situation we can always use a dictionary(map).\r\n",
    "\r\n",
    "But instead such typical data structures, we can use our advanced data stuctures knowledge into use here by using tree like data structure. This option gives faster access/search times as well as good compression.\r\n",
    "\r\n",
    "\r\n",
    "**Directed Acyclic Word Graph (DAWG)**\r\n",
    "\r\n",
    "This is a very interesting implementation of graph based data structure to store/compress string. The storing method can be different based on the implementation. But idea is each charater have its own node and tokens means connections between such nodes. This dramatically reduce the memory consumption and reduce the search time as well..\r\n",
    "\r\n",
    "You can read more details about it [here](https://dawg.readthedocs.io/en/latest/).\r\n",
    "\r\n",
    "\r\n",
    "**Tries**\r\n",
    "\r\n",
    "Another data structure we can use to store string data is tries. Since it behaves simialar to trees we can store characters in nodes and their connections. Since in a language like English considerable amount of strings have similar prefixes, tries can help greatly to reduce memory consumption while keeping the processing speed. But it is worth noting that DAWGs can compress better compared to tries. In python `Marisa Tries` is a package, we can use for this purpose.\r\n",
    "\r\n",
    "\r\n",
    "> Other than those we can use probabilistic data structures to save values as well. This causes our values to be less accurate but with incredible memory savings. Example for such data structures a `Morris Counters`, `K-min Values`, `Bloom Filters`, `LogLog Counter` etc. These have their specific qualities and therefore usage is highly dependent on usecases."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('High_Performance_Concepts_in_Python-45__Ubn2': virtualenv)"
  },
  "interpreter": {
   "hash": "7e345707fd4776d029d2a8a839d91a2c00b4b8e78ea36281d84f354477ba90d2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}