{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Multiprocessing with Python**\r\n",
    "\r\n",
    "Python is inherently not designed for multicore works. It has something called GIL (global interpreter lock) which causes all the threads to act in serial manner when interacting with python interpreter. But this does not mean that there are no ways to make multicore programs in python.\r\n",
    "\r\n",
    "But there are few things to note before diving in to multiprocessing.\r\n",
    "\r\n",
    "1. Speedup is not linear with cores/threads used. There are overheads atatched to multiprocessing. (as any other language of course)\r\n",
    "2. Shared  states between threads means more annoyance to handle, which means high overhead/ development effort.\r\n",
    "3. Program will get affected by Amdahl's law. if dont remember read!\r\n",
    "4. Python threads are not like other languages (Java, C++). They are literal threads as in OS native threads, but they all will work in single process which means one python interpreter. So will get bottlenecked by the GIL. Therefore we use processes with independent interpreters for each.\r\n",
    "5. Should reduce message/state passing between processes as much as possible."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Python Multiprocessing overview\r\n",
    "\r\n",
    "* Process - A forked copy of current process with new identifier. We can provide a target method to run in it.\r\n",
    "* Pool - Wrapper for process or threading. \r\n",
    "* Queue - A FIFO queue for multiple producer/consumer pattern\r\n",
    "* Pipe - A communication channel between 2 processes.\r\n",
    "* Manager - A high level managed interface to share objects between processes.\r\n",
    "\r\n",
    "\r\n",
    "For the example usage of multiprocessing, we will use Monte Carlo simulation of Pi calculation (learned in stats). Below is the base implementation of experiment unit (random x, y values in unit area)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "%%writefile Multiprocessing/pi_estimate.py\r\n",
    "\r\n",
    "import os\r\n",
    "import random\r\n",
    "\r\n",
    "def calc_point_inside_circle(num_of_estimates):\r\n",
    "    \r\n",
    "    print(f\"Executing calc_point_inside_circle with {num_of_estimates:,} on pid {os.getpid()}\")\r\n",
    "\r\n",
    "    trials_inside_circle = 0\r\n",
    "\r\n",
    "    for step in range(int(num_of_estimates)):\r\n",
    "        x = random.uniform(0,1)\r\n",
    "        y = random.uniform(0,1)\r\n",
    "\r\n",
    "        is_inside_circle = 1 if (x**2 + y**2) <= 1 else 0\r\n",
    "        trials_inside_circle += is_inside_circle\r\n",
    "\r\n",
    "    return trials_inside_circle\r\n",
    "\r\n",
    "\r\n",
    "from multiprocessing import Pool # This is process based\r\n",
    "# from multiprocessing import Pool # This is thread based\r\n",
    "import time\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    total_trials = 1e8\r\n",
    "    num_workers = 4\r\n",
    "\r\n",
    "    pool = Pool(processes=num_workers)\r\n",
    "    trials_per_worker = total_trials/num_workers\r\n",
    "    trials_per_processes = [trials_per_worker]*num_workers\r\n",
    "\r\n",
    "    start_time = time.time()\r\n",
    "    trials_inside_circle = pool.map(calc_point_inside_circle, trials_per_processes)\r\n",
    "    pi_estimate = (sum(trials_inside_circle)*4)/float(total_trials)\r\n",
    "    print(pi_estimate)\r\n",
    "    print(f\"Time consumed: {time.time()-start_time}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting Multiprocessing/pi_estimate.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "If we run the above segment in 4 processes output is as follows.\r\n",
    "\r\n",
    "<center><image src=\"./img/15.jpg\" width=\"700\"/></center>\r\n",
    "\r\n",
    "If we only used 1 process output would be like below.\r\n",
    "\r\n",
    "<center><image src=\"./img/16.jpg\" width=\"600\"/></center>\r\n",
    "\r\n",
    "We can see the clear performance improvement by using multiple processes for this operation. But instead if we used threads we cant expect the same amount of performance gain due to GIL.\r\n",
    "\r\n",
    "## Python Joblib module\r\n",
    "\r\n",
    "Joblib is an improvement on the multioprocessing module, with lightweight pipelining. It can easily be used in pure python/numpy processes with embarasingly parrallel preperty. Also this can be used in calling expensive functions where outputs can be cached to disk between sessions.\r\n",
    "Install joblib package using below.\r\n",
    "\r\n",
    "<center>pip install joblib</center>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "%%writefile Multiprocessing/pi_estimate_joblib.py\r\n",
    "\r\n",
    "import os\r\n",
    "import random\r\n",
    "import time\r\n",
    "\r\n",
    "def calc_point_inside_circle(num_of_estimates):\r\n",
    "    \r\n",
    "    print(f\"Executing calc_point_inside_circle with {num_of_estimates:,} on pid {os.getpid()}\")\r\n",
    "\r\n",
    "    trials_inside_circle = 0\r\n",
    "\r\n",
    "    for step in range(int(num_of_estimates)):\r\n",
    "        x = random.uniform(0,1)\r\n",
    "        y = random.uniform(0,1)\r\n",
    "\r\n",
    "        is_inside_circle = 1 if (x**2 + y**2) <= 1 else 0\r\n",
    "        trials_inside_circle += is_inside_circle\r\n",
    "\r\n",
    "    return trials_inside_circle\r\n",
    "\r\n",
    "from joblib import Parallel, delayed\r\n",
    "if __name__ == \"__main__\":\r\n",
    "\r\n",
    "    total_trials = 1e8\r\n",
    "    num_workers = 4\r\n",
    "\r\n",
    "    trials_per_worker = total_trials/num_workers\r\n",
    "    trials_per_processes = [trials_per_worker]*num_workers\r\n",
    "\r\n",
    "\r\n",
    "    parrallel_obj = Parallel(n_jobs=num_workers, verbose=1)\r\n",
    "    async_function = delayed(calc_point_inside_circle)\r\n",
    "\r\n",
    "    start_time = time.time()\r\n",
    "    trials_inside_circle =  parrallel_obj(async_function(trials_per_worker) for _ in range(num_workers))\r\n",
    "    pi_estimate = (sum(trials_inside_circle)*4)/float(total_trials)\r\n",
    "\r\n",
    "    print(pi_estimate)\r\n",
    "    print(f\"Time consumed: {time.time()-start_time}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting Multiprocessing/pi_estimate_joblib.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Joblib function syntax is bit confusing as it behave like a chain. Anyhow the output is as follows.\r\n",
    "\r\n",
    "<center><image src=\"./img/17.jpg\" width=\"600\"/></center>\r\n",
    "\r\n",
    "Total consumed time is less than just using a one process.\r\n",
    "\r\n",
    "Parrallel class have so many parameter which we can play with including debug info, timeouts, change usage to threads instead of processes and change the backend etc. Can mess around with those based on the requirement you have.\r\n",
    "\r\n",
    "Another useful feature in joblib is it's `MemoryCache`. This decorator function saves the results to disk cache based on the input arguments. But this require the async_function to have unique arguments. Otherwise cache store wont be able to uniquely identify the results from each process. To do that we can add an index as additional parameter. Check the code below.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "%%writefile Multiprocessing/pi_estimate_joblib_cached.py\r\n",
    "\r\n",
    "import os\r\n",
    "import random\r\n",
    "import time\r\n",
    "from joblib import Parallel, delayed\r\n",
    "from joblib import Memory\r\n",
    "memory = Memory(\"./Multiprocessing/joblib_cache\", verbose=0)\r\n",
    "\r\n",
    "@memory.cache\r\n",
    "def calc_point_inside_circle_with_idx(num_of_estimates, idx):\r\n",
    "    \r\n",
    "    print(f\"Executing calc_point_inside_circle with {num_of_estimates:,} and index {idx} on pid {os.getpid()}\")\r\n",
    "\r\n",
    "    trials_inside_circle = 0\r\n",
    "\r\n",
    "    for step in range(int(num_of_estimates)):\r\n",
    "        x = random.uniform(0,1)\r\n",
    "        y = random.uniform(0,1)\r\n",
    "\r\n",
    "        is_inside_circle = 1 if (x**2 + y**2) <= 1 else 0\r\n",
    "        trials_inside_circle += is_inside_circle\r\n",
    "\r\n",
    "    return trials_inside_circle\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "\r\n",
    "    total_trials = 1e8\r\n",
    "    num_workers = 4\r\n",
    "\r\n",
    "    trials_per_worker = total_trials/num_workers\r\n",
    "    trials_per_processes = [trials_per_worker]*num_workers\r\n",
    "\r\n",
    "\r\n",
    "    parrallel_obj = Parallel(n_jobs=num_workers, verbose=1)\r\n",
    "    async_function = delayed(calc_point_inside_circle_with_idx)\r\n",
    "\r\n",
    "    start_time = time.time()\r\n",
    "    trials_inside_circle =  parrallel_obj(async_function(trials_per_worker, i) for i in range(num_workers))\r\n",
    "    pi_estimate = (sum(trials_inside_circle)*4)/float(total_trials)\r\n",
    "\r\n",
    "    print(pi_estimate)\r\n",
    "    print(f\"Time consumed: {time.time()-start_time}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting Multiprocessing/pi_estimate_joblib_cached.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we run the above code for the first time output is as follows.\r\n",
    "\r\n",
    "<center><image src=\"./img/18.jpg\" width=\"600\"/></center>\r\n",
    "\r\n",
    "But if we did not change the arguments, then results will come from the cache improving the performance significantly.\r\n",
    "\r\n",
    "<center><image src=\"./img/19.jpg\" width=\"600\"/></center>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random number generation in parallel processes\r\n",
    "\r\n",
    "Generating random numbers is a weird problem itself, and if we try to do it in parallel processes things get complicated bit more as there may be repeating patterns, correlations between numbers generated by the processes. \r\n",
    "\r\n",
    "> If we use normal python random module with multiprocessing, they will automatically get seeded with different values which would yield unique sequence for each process. But if we used numpy random generators, we need to explicitly set the seeds. Otherwise each process would have same random sequence.\r\n",
    "\r\n",
    "Below is example numpy based implementation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "%%writefile Multiprocessing/pi_estimate_joblib_cached_numpy.py\r\n",
    "\r\n",
    "import os\r\n",
    "import random\r\n",
    "import time\r\n",
    "from joblib import Parallel, delayed\r\n",
    "from joblib import Memory\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "memory = Memory(\"./Multiprocessing/joblib_cache\", verbose=0)\r\n",
    "\r\n",
    "@memory.cache\r\n",
    "def calc_point_inside_circle_with_numpy(num_of_estimates, idx):\r\n",
    "    \r\n",
    "    print(f\"Executing calc_point_inside_circle with {num_of_estimates:,} and index {idx} on pid {os.getpid()}\")\r\n",
    "    import numpy as np\r\n",
    "    np.random.seed()\r\n",
    "\r\n",
    "    xs = np.random.uniform(0, 1, num_of_estimates)\r\n",
    "    ys = np.random.uniform(0, 1, num_of_estimates)\r\n",
    "\r\n",
    "    sqr_zs = (xs*xs + ys*ys) <= 1 # returns a boolean array with condition checked\r\n",
    "\r\n",
    "    trials_inside_circle = np.sum(sqr_zs)\r\n",
    "    return trials_inside_circle\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "\r\n",
    "    total_trials = 1e8\r\n",
    "    num_workers = 4\r\n",
    "\r\n",
    "    trials_per_worker = int(total_trials/num_workers)\r\n",
    "    trials_per_processes = [trials_per_worker]*num_workers\r\n",
    "\r\n",
    "\r\n",
    "    parrallel_obj = Parallel(n_jobs=num_workers, verbose=1)\r\n",
    "    async_function = delayed(calc_point_inside_circle_with_numpy)\r\n",
    "\r\n",
    "    start_time = time.time()\r\n",
    "    trials_inside_circle =  parrallel_obj(async_function(trials_per_worker, i) for i in range(num_workers))\r\n",
    "    pi_estimate = (np.sum(trials_inside_circle)*4)/float(total_trials)\r\n",
    "\r\n",
    "    print(pi_estimate)\r\n",
    "    print(f\"Time consumed: {time.time()-start_time}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting Multiprocessing/pi_estimate_joblib_cached_numpy.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the output of the above example code. See the improvement gained by numpy operations.\r\n",
    "\r\n",
    "<center><image src=\"./img/20.jpg\" width=\"600\"/></center>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prime Number search\r\n",
    "\r\n",
    "Another sample example for using parrallel processing is finding prime numbers in a given range. This is a computationally expensive process specially if the number is large. Also because of that, even if we run the tasks parrallely tasks containing large numbers would take more time. Therefore different type of scheduling queue or task assigning method would be more beneficial.\r\n",
    "\r\n",
    "To do that we can use `multiprocessing queues`. These keep objects which can be shared across multiple processes. They are synchronized and non persistent which means there are overheads in using and will loose data if program crashed.\r\n",
    "\r\n",
    "These multiprocessing queues use picked objects to share data. Keep that in mind when developing programs as it could be a bottleneck."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import math\r\n",
    "\r\n",
    "def check_prime_serial(n):\r\n",
    "    if n % 2 == 0:\r\n",
    "        return False\r\n",
    "    for i in range(3, int(math.sqrt(n)) + 1, 2):\r\n",
    "        if n % i == 0:\r\n",
    "            return False\r\n",
    "    return True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below is the implementation of above check prime function using queues. \r\n",
    "\r\n",
    "**Very similar to Pub-Sub architecture in software development**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "%%writefile Multiprocessing/prime_search_queues.py\r\n",
    "\r\n",
    "import multiprocessing\r\n",
    "from multiprocessing import Pool\r\n",
    "import time\r\n",
    "import math\r\n",
    "\r\n",
    "FLAG_ALL_DONE = b\"WORK_FINISHED\"\r\n",
    "FLAG_WORKER_FINISHED_PROCESSING = b\"WORKER_FINISHED_PROCESSING\"\r\n",
    "\r\n",
    "def check_prime(search_space_queue, verified_primes_queue):\r\n",
    "\r\n",
    "    while True:\r\n",
    "        n = search_space_queue.get() # This is a blocking operation.\r\n",
    "\r\n",
    "        if n == FLAG_ALL_DONE:\r\n",
    "            # flag that our results have all been pushed to the results queue\r\n",
    "            verified_primes_queue.put(FLAG_WORKER_FINISHED_PROCESSING)\r\n",
    "            break\r\n",
    "        else:\r\n",
    "            if n % 2 == 0:\r\n",
    "                continue\r\n",
    "            for i in range(3, int(math.sqrt(n)) + 1, 2):\r\n",
    "                if n % i == 0:\r\n",
    "                    break\r\n",
    "            else:\r\n",
    "                verified_primes_queue.put(n)\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "\r\n",
    "    num_of_workers = 4\r\n",
    "\r\n",
    "    primes = []\r\n",
    "    manager = multiprocessing.Manager()\r\n",
    "    search_space_queue = manager.Queue()\r\n",
    "    verified_primes_queue = manager.Queue()\r\n",
    "\r\n",
    "    pool = Pool(processes=num_of_workers)\r\n",
    "\r\n",
    "    # Initializing the child processes.\r\n",
    "    processes = []\r\n",
    "    for _ in range(num_of_workers):\r\n",
    "        p = multiprocessing.Process(target=check_prime, \r\n",
    "                                    args=(  search_space_queue, \r\n",
    "                                            verified_primes_queue)\r\n",
    "                                    )\r\n",
    "        processes.append(p)\r\n",
    "        p.start()\r\n",
    "\r\n",
    "    # Actual calculations begin here!\r\n",
    "    t1 = time.time()\r\n",
    "    number_range = range(100_000_000, 101_000_000)\r\n",
    "\r\n",
    "    # add jobs to the inbound work queue\r\n",
    "    for number in number_range:\r\n",
    "        search_space_queue.put(number)\r\n",
    "\r\n",
    "    # add poison pills to stop the remote workers(force to break from the infinite loop)\r\n",
    "    for n in range(num_of_workers):\r\n",
    "        search_space_queue.put(FLAG_ALL_DONE)\r\n",
    "\r\n",
    "    # Now wait till child processes complete their tasks.\r\n",
    "    finished_child_processes = 0\r\n",
    "    while True:\r\n",
    "        new_result = verified_primes_queue.get() # Wait till a child process put a result to the queue\r\n",
    "        if new_result == FLAG_WORKER_FINISHED_PROCESSING:\r\n",
    "            finished_child_processes += 1\r\n",
    "            if finished_child_processes == num_of_workers:\r\n",
    "                break\r\n",
    "        else:\r\n",
    "            primes.append(new_result)\r\n",
    "\r\n",
    "    assert finished_child_processes == num_of_workers\r\n",
    "\r\n",
    "    print(\"Took:\", time.time() - t1)\r\n",
    "    print(len(primes), primes[:10], primes[-10:])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting Multiprocessing/prime_search_queues.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here 2 flags are defined to control the behaviour of the task. The `FLAG_ALL_DONE` is called poison pill because it causes the process to exit the loop once recognized. It is fed by the parent process and once every value is processed, it will act as a sentinel to stop further processing. Second flag, `FLAG_WORKER_FINISHED_PROCESSING` is controlled by the child processes. \r\n",
    "\r\n",
    "Above program outout is as below.\r\n",
    "\r\n",
    "<center><image src=\"./img/21.jpg\" width=\"600\"/></center>\r\n",
    "\r\n",
    "As you can see this takes considerable amount of time. If we used just one process output is as below.\r\n",
    "\r\n",
    "<center><image src=\"./img/22.jpg\" width=\"600\"/></center>\r\n",
    "\r\n",
    "Here time consumption is less than 4 processes. Reason for that is using queues have large overhead because of locking mechanisms, pickling/unpickling processes etc. But our problem is not large enough to gain any benefit from using this type of implementation.\r\n",
    "\r\n",
    "> If your task has a long completion time (at least a sizable fraction of a second) with a small amount of communication, a Queue approach might be the right answer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interprocess Communication\r\n",
    "\r\n",
    "In order to check the python interprocess communication behaviour, we will check the task of verifying whether the given number is prime. In prime number checking it becomes time consuming when the numbers getting larger as larger numbers can have more factors. Therefore using multiple processes to do this task can be more efficient.\r\n",
    "\r\n",
    "Below include various IPC methods used for the verification of prime numbers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "from math import sqrt\r\n",
    "\r\n",
    "def check_prime_serial(n):\r\n",
    "\r\n",
    "    if(n%2==0):\r\n",
    "        return False\r\n",
    "    for i in range(3, int(sqrt(n)+1), 2):\r\n",
    "        if(n%i==0):\r\n",
    "            return False\r\n",
    "    return True\r\n",
    "\r\n",
    "%timeit check_prime_serial(112272535095295)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "587 ns ± 1.65 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "%%writefile Multiprocessing/verify_prime_naive.py\r\n",
    "\r\n",
    "import multiprocessing\r\n",
    "from multiprocessing import Pool\r\n",
    "import time\r\n",
    "import math\r\n",
    "\r\n",
    "def create_range(from_i, to_i, num_processes):\r\n",
    "\r\n",
    "    n = to_i - from_i\r\n",
    "    chunk = (n/num_processes)\r\n",
    "\r\n",
    "    output = []\r\n",
    "    for i in range(num_processes):\r\n",
    "        output.append((from_i+int(chunk*i), from_i+int(chunk*(i+1))))\r\n",
    "    \r\n",
    "    return output\r\n",
    "\r\n",
    "def check_prime(n, pool, num_processes):\r\n",
    "    from_i = 3\r\n",
    "    to_i = int(math.sqrt(n)) + 1\r\n",
    "    ranges_to_check = create_range(from_i, to_i, num_processes)\r\n",
    "    ranges_to_check = zip(num_processes*[n], ranges_to_check)\r\n",
    "\r\n",
    "    results = pool.map(check_prime_in_range, ranges_to_check)\r\n",
    "    if False in results:\r\n",
    "        return False\r\n",
    "    return True\r\n",
    "\r\n",
    "def check_prime_in_range(n_from_i_to_i):\r\n",
    "    (n, (from_i, to_i)) = n_from_i_to_i\r\n",
    "    if n % 2 == 0:\r\n",
    "        return False\r\n",
    "    if (from_i % 2 == 0):\r\n",
    "        from_i = from_i - 1\r\n",
    "    for i in range(from_i, int(to_i), 2):\r\n",
    "        if n % i == 0:\r\n",
    "            return False\r\n",
    "    return True\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    #[non_prime, non_prime, non_prime, prime, prime]\r\n",
    "    test_cases = [112272535095295, 100109100129100369, 100109100129101027, 100109100129100151, 100109100129162907]\r\n",
    "\r\n",
    "    num_of_workers = 4\r\n",
    "\r\n",
    "    primes = []\r\n",
    "    manager = multiprocessing.Manager()\r\n",
    "    \r\n",
    "   \r\n",
    "\r\n",
    "    for n in test_cases:\r\n",
    "        pool = Pool(processes=num_of_workers)\r\n",
    "        st = time.time()\r\n",
    "        print(f'The Value {n} is a prime: {check_prime(n, pool, num_of_workers)} (Took {time.time() - st} seconds')\r\n",
    "        \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting Multiprocessing/verify_prime_naive.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output of above naive implementation is as follows.\r\n",
    "\r\n",
    "<center><image src=\"./img/23.jpg\" width=\"500\"/></center>\r\n",
    "\r\n",
    "Here in small non_prime case, the time it took it is bit large compared to the pure serial implementation (587 ns ± 1.65 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)). This is due to various overheads in parrallel processing. We can make this solution better by making a serial check for first smaller digit section. Sample implementation is as below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "%%writefile Multiprocessing/verify_prime_less_naive.py\r\n",
    "\r\n",
    "import multiprocessing\r\n",
    "from multiprocessing import Pool\r\n",
    "import time\r\n",
    "import math\r\n",
    "\r\n",
    "def create_range(from_i, to_i, num_processes):\r\n",
    "\r\n",
    "    n = to_i - from_i\r\n",
    "    chunk = (n/num_processes)\r\n",
    "\r\n",
    "    output = []\r\n",
    "    for i in range(num_processes):\r\n",
    "        output.append((from_i+int(chunk*i), from_i+int(chunk*(i+1))))\r\n",
    "    \r\n",
    "    return output\r\n",
    "\r\n",
    "def check_prime(n, pool, num_processes):\r\n",
    "    from_i = 3\r\n",
    "    to_i = int(math.sqrt(n)) + 1\r\n",
    "\r\n",
    "    result = check_prime_in_range((n, (from_i, min(to_i, 21))))\r\n",
    "    if to_i<=21 or result==False:\r\n",
    "        return result\r\n",
    "    \r\n",
    "    from_i = 21\r\n",
    "    ranges_to_check = create_range(from_i, to_i, num_processes)\r\n",
    "    ranges_to_check = zip(num_processes*[n], ranges_to_check)\r\n",
    "\r\n",
    "    results = pool.map(check_prime_in_range, ranges_to_check)\r\n",
    "    if False in results:\r\n",
    "        return False\r\n",
    "    return True\r\n",
    "\r\n",
    "def check_prime_in_range(n_from_i_to_i):\r\n",
    "    (n, (from_i, to_i)) = n_from_i_to_i\r\n",
    "    if n % 2 == 0:\r\n",
    "        return False\r\n",
    "    if (from_i % 2 == 0):\r\n",
    "        from_i = from_i - 1\r\n",
    "    for i in range(from_i, int(to_i), 2):\r\n",
    "        if n % i == 0:\r\n",
    "            return False\r\n",
    "    return True\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    #[non_prime, non_prime, non_prime, prime, prime]\r\n",
    "    test_cases = [112272535095295, 100109100129100369, 100109100129101027, 100109100129100151, 100109100129162907]\r\n",
    "\r\n",
    "    num_of_workers = 4\r\n",
    "\r\n",
    "    primes = []\r\n",
    "    manager = multiprocessing.Manager()\r\n",
    "    \r\n",
    "    for n in test_cases:\r\n",
    "        pool = Pool(processes=num_of_workers)\r\n",
    "        st = time.time()\r\n",
    "        print(f'The Value {n} is a prime: {check_prime(n, pool, num_of_workers)} (Took {time.time() - st} seconds)')\r\n",
    "        \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting Multiprocessing/verify_prime_less_naive.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "See the speedup we gain for the first non prime number.\r\n",
    "\r\n",
    "<center><image src=\"./img/24.jpg\" width=\"500\"/></center>\r\n",
    "\r\n",
    "Serialization of code segments as a precheck condition is a common technique used in parallel computing to avoid overheads associated with the parallelization.\r\n",
    "\r\n",
    "One major issue in above implementation is that, even though a processes exited after realizing the number is a prime, other processes will still continue to search in their assigned range since they do not communicate with each other. To solve that issue we can use a flag like variable common for all the processes. This flag provided by `multiprocessing.Manager` class is bit uninuitive in my opinion as of now. Any how usage of it is below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "%%writefile Multiprocessing/verify_prime_flagged.py\r\n",
    "\r\n",
    "import multiprocessing\r\n",
    "from multiprocessing import Pool\r\n",
    "import time\r\n",
    "import math\r\n",
    "\r\n",
    "def create_range(from_i, to_i, num_processes):\r\n",
    "\r\n",
    "    n = to_i - from_i\r\n",
    "    chunk = (n/num_processes)\r\n",
    "\r\n",
    "    output = []\r\n",
    "    for i in range(num_processes):\r\n",
    "        output.append((from_i+int(chunk*i), from_i+int(chunk*(i+1))))\r\n",
    "    \r\n",
    "    return output\r\n",
    "\r\n",
    "def check_prime(n, pool, num_processes, flag):\r\n",
    "    from_i = 3\r\n",
    "    to_i = int(math.sqrt(n)) + 1\r\n",
    "    flag.value = FLAG_CLEAR\r\n",
    "\r\n",
    "    result = check_prime_in_range( (n, (from_i, min(to_i, SERIAL_CHECK_CUTOFF)), flag) )\r\n",
    "    if to_i<=SERIAL_CHECK_CUTOFF or result==False:\r\n",
    "        return result\r\n",
    "    \r\n",
    "    from_i = SERIAL_CHECK_CUTOFF\r\n",
    "    ranges_to_check = create_range(from_i, to_i, num_processes)\r\n",
    "    ranges_to_check = zip(num_processes*[n], ranges_to_check, num_processes*[flag])\r\n",
    "\r\n",
    "    results = pool.map(check_prime_in_range, ranges_to_check)\r\n",
    "    if False in results:\r\n",
    "        return False\r\n",
    "    return True\r\n",
    "\r\n",
    "def check_prime_in_range(n_from_i_to_i_flag):\r\n",
    "    (n, (from_i, to_i), flag) = n_from_i_to_i_flag\r\n",
    "    if n % 2 == 0:\r\n",
    "        return False\r\n",
    "    if (from_i % 2 == 0):\r\n",
    "        from_i = from_i - 1\r\n",
    "\r\n",
    "    check_freq = CHECK_EVERY\r\n",
    "    for i in range(from_i, int(to_i), 2):\r\n",
    "        if(i%check_freq==0):\r\n",
    "            if(flag.value==FLAG_SET):\r\n",
    "                return False\r\n",
    "        if n % i == 0:\r\n",
    "            flag.value = FLAG_SET\r\n",
    "            return False\r\n",
    "    return True\r\n",
    "\r\n",
    "\r\n",
    "SERIAL_CHECK_CUTOFF = 21\r\n",
    "CHECK_EVERY = 1000\r\n",
    "FLAG_CLEAR = b'0'\r\n",
    "FLAG_SET = b'1'\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    #[non_prime, non_prime, non_prime, prime, prime]\r\n",
    "    test_cases = [112272535095295, 100109100129100369, 100109100129101027, 100109100129100151, 100109100129162907]\r\n",
    "\r\n",
    "    num_of_workers = 4\r\n",
    "\r\n",
    "    primes = []\r\n",
    "    manager = multiprocessing.Manager()\r\n",
    "    flag = manager.Value(b'c', FLAG_CLEAR) # Setting the initial flag\r\n",
    "    \r\n",
    "    for n in test_cases:\r\n",
    "        pool = Pool(processes=num_of_workers)\r\n",
    "        st = time.time()\r\n",
    "        print(f'The Value {n} is a prime: {check_prime(n, pool, num_of_workers, flag)} (Took {time.time() - st} seconds)')\r\n",
    "        \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting Multiprocessing/verify_prime_flagged.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output is as follows.\r\n",
    "\r\n",
    "<center><image src=\"./img/25.jpg\" width=\"500\"/></center>\r\n",
    "\r\n",
    "As we can observe, execution speed is not great because of the additional overhead of reading a shared object. Therefore usage of this technique will greatly depend on your task.\r\n",
    "\r\n",
    "\r\n",
    "__Instead of using multiprocessing library provided `Manager.Value` we can use key/value store `Redis` which has its own concurrent handling mechanism as a shared value store. This is great because then shared values can be used with any programming language/tool.__\r\n",
    "\r\n",
    "\r\n",
    "> Redis stores everything in RAM and snapshots to disk (optionally using journaling) and supports master/slave replication to a cluster of instances.\r\n",
    "\r\n",
    "Also there are several other python methods/modules to share objects among processes. Which include `mmap`, `fastners` module etc. They have their own specialities and weaknesses. For example `mmap` is not synchronizing. Therefore multiple updations at the same time may cause data corruptions. But in a usecase like checking for primes we dont need synchronization. Therefore can use mmaps to solve our problem."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('High_Performance_Concepts_in_Python-45__Ubn2': virtualenv)"
  },
  "interpreter": {
   "hash": "7e345707fd4776d029d2a8a839d91a2c00b4b8e78ea36281d84f354477ba90d2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}