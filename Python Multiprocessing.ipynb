{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Multiprocessing with Python**\r\n",
    "\r\n",
    "Python is inherently not designed for multicore works. It has something called GIL (global interpreter lock) which causes all the threads to act in serial manner when interacting with python interpreter. But this does not mean that there are no ways to make multicore programs in python.\r\n",
    "\r\n",
    "But there are few things to note before diving in to multiprocessing.\r\n",
    "\r\n",
    "1. Speedup is not linear with cores/threads used. There are overheads atatched to multiprocessing. (as any other language of course)\r\n",
    "2. Shared  states between threads means more annoyance to handle, which means high overhead/ development effort.\r\n",
    "3. Program will get affected by Amdahl's law. if dont remember read!\r\n",
    "4. Python threads are not like other languages (Java, C++). They are literal threads as in OS native threads, but they all will work in single process which means one python interpreter. So will get bottlenecked by the GIL. Therefore we use processes with independent interpreters for each.\r\n",
    "5. Should reduce message/state passing between processes as much as possible."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Python Multiprocessing overview\r\n",
    "\r\n",
    "* Process - A forked copy of current process with new identifier. We can provide a target method to run in it.\r\n",
    "* Pool - Wrapper for process or threading. \r\n",
    "* Queue - A FIFO queue for multiple producer/consumer pattern\r\n",
    "* Pipe - A communication channel between 2 processes.\r\n",
    "* Manager - A high level managed interface to share objects between processes.\r\n",
    "\r\n",
    "\r\n",
    "For the example usage of multiprocessing, we will use Monte Carlo simulation of Pi calculation (learned in stats). Below is the base implementation of experiment unit (random x, y values in unit area)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "%%writefile Multiprocessing/pi_estimate.py\r\n",
    "\r\n",
    "import os\r\n",
    "import random\r\n",
    "\r\n",
    "def calc_point_inside_circle(num_of_estimates):\r\n",
    "    \r\n",
    "    print(f\"Executing calc_point_inside_circle with {num_of_estimates:,} on pid {os.getpid()}\")\r\n",
    "\r\n",
    "    trials_inside_circle = 0\r\n",
    "\r\n",
    "    for step in range(int(num_of_estimates)):\r\n",
    "        x = random.uniform(0,1)\r\n",
    "        y = random.uniform(0,1)\r\n",
    "\r\n",
    "        is_inside_circle = 1 if (x**2 + y**2) <= 1 else 0\r\n",
    "        trials_inside_circle += is_inside_circle\r\n",
    "\r\n",
    "    return trials_inside_circle\r\n",
    "\r\n",
    "\r\n",
    "from multiprocessing import Pool # This is process based\r\n",
    "# from multiprocessing import Pool # This is thread based\r\n",
    "import time\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    total_trials = 1e8\r\n",
    "    num_workers = 4\r\n",
    "\r\n",
    "    pool = Pool(processes=num_workers)\r\n",
    "    trials_per_worker = total_trials/num_workers\r\n",
    "    trials_per_processes = [trials_per_worker]*num_workers\r\n",
    "\r\n",
    "    start_time = time.time()\r\n",
    "    trials_inside_circle = pool.map(calc_point_inside_circle, trials_per_processes)\r\n",
    "    pi_estimate = (sum(trials_inside_circle)*4)/float(total_trials)\r\n",
    "    print(pi_estimate)\r\n",
    "    print(f\"Time consumed: {time.time()-start_time}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting Multiprocessing/pi_estimate.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "If we run the above segment in 4 processes output is as follows.\r\n",
    "\r\n",
    "<center><image src=\"./img/15.jpg\" width=\"700\"/></center>\r\n",
    "\r\n",
    "If we only used 1 process output would be like below.\r\n",
    "\r\n",
    "<center><image src=\"./img/16.jpg\" width=\"600\"/></center>\r\n",
    "\r\n",
    "We can see the clear performance improvement by using multiple processes for this operation. But instead if we used threads we cant expect the same amount of performance gain due to GIL.\r\n",
    "\r\n",
    "## Python Joblib module\r\n",
    "\r\n",
    "Joblib is an improvement on the multioprocessing module, with lightweight pipelining. It can easily be used in pure python/numpy processes with embarasingly parrallel preperty. Also this can be used in calling expensive functions where outputs can be cached to disk between sessions.\r\n",
    "Install joblib package using below.\r\n",
    "\r\n",
    "<center>pip install joblib</center>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "%%writefile Multiprocessing/pi_estimate_joblib.py\r\n",
    "\r\n",
    "import os\r\n",
    "import random\r\n",
    "import time\r\n",
    "\r\n",
    "def calc_point_inside_circle(num_of_estimates):\r\n",
    "    \r\n",
    "    print(f\"Executing calc_point_inside_circle with {num_of_estimates:,} on pid {os.getpid()}\")\r\n",
    "\r\n",
    "    trials_inside_circle = 0\r\n",
    "\r\n",
    "    for step in range(int(num_of_estimates)):\r\n",
    "        x = random.uniform(0,1)\r\n",
    "        y = random.uniform(0,1)\r\n",
    "\r\n",
    "        is_inside_circle = 1 if (x**2 + y**2) <= 1 else 0\r\n",
    "        trials_inside_circle += is_inside_circle\r\n",
    "\r\n",
    "    return trials_inside_circle\r\n",
    "\r\n",
    "from joblib import Parallel, delayed\r\n",
    "if __name__ == \"__main__\":\r\n",
    "\r\n",
    "    total_trials = 1e8\r\n",
    "    num_workers = 4\r\n",
    "\r\n",
    "    trials_per_worker = total_trials/num_workers\r\n",
    "    trials_per_processes = [trials_per_worker]*num_workers\r\n",
    "\r\n",
    "\r\n",
    "    parrallel_obj = Parallel(n_jobs=num_workers, verbose=1)\r\n",
    "    async_function = delayed(calc_point_inside_circle)\r\n",
    "\r\n",
    "    start_time = time.time()\r\n",
    "    trials_inside_circle =  parrallel_obj(async_function(trials_per_worker) for _ in range(num_workers))\r\n",
    "    pi_estimate = (sum(trials_inside_circle)*4)/float(total_trials)\r\n",
    "\r\n",
    "    print(pi_estimate)\r\n",
    "    print(f\"Time consumed: {time.time()-start_time}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting Multiprocessing/pi_estimate_joblib.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Joblib function syntax is bit confusing as it behave like a chain. Anyhow the output is as follows.\r\n",
    "\r\n",
    "<center><image src=\"./img/17.jpg\" width=\"600\"/></center>\r\n",
    "\r\n",
    "Total consumed time is less than just using a one process.\r\n",
    "\r\n",
    "Parrallel class have so many parameter which we can play with including debug info, timeouts, change usage to threads instead of processes and change the backend etc. Can mess around with those based on the requirement you have.\r\n",
    "\r\n",
    "Another useful feature in joblib is it's `MemoryCache`. This decorator function saves the results to disk cache based on the input arguments. But this require the async_function to have unique arguments. Otherwise cache store wont be able to uniquely identify the results from each process. To do that we can add an index as additional parameter. Check the code below.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "%%writefile Multiprocessing/pi_estimate_joblib_cached.py\r\n",
    "\r\n",
    "import os\r\n",
    "import random\r\n",
    "import time\r\n",
    "from joblib import Parallel, delayed\r\n",
    "from joblib import Memory\r\n",
    "memory = Memory(\"./Multiprocessing/joblib_cache\", verbose=0)\r\n",
    "\r\n",
    "@memory.cache\r\n",
    "def calc_point_inside_circle_with_idx(num_of_estimates, idx):\r\n",
    "    \r\n",
    "    print(f\"Executing calc_point_inside_circle with {num_of_estimates:,} and index {idx} on pid {os.getpid()}\")\r\n",
    "\r\n",
    "    trials_inside_circle = 0\r\n",
    "\r\n",
    "    for step in range(int(num_of_estimates)):\r\n",
    "        x = random.uniform(0,1)\r\n",
    "        y = random.uniform(0,1)\r\n",
    "\r\n",
    "        is_inside_circle = 1 if (x**2 + y**2) <= 1 else 0\r\n",
    "        trials_inside_circle += is_inside_circle\r\n",
    "\r\n",
    "    return trials_inside_circle\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "\r\n",
    "    total_trials = 1e8\r\n",
    "    num_workers = 4\r\n",
    "\r\n",
    "    trials_per_worker = total_trials/num_workers\r\n",
    "    trials_per_processes = [trials_per_worker]*num_workers\r\n",
    "\r\n",
    "\r\n",
    "    parrallel_obj = Parallel(n_jobs=num_workers, verbose=1)\r\n",
    "    async_function = delayed(calc_point_inside_circle_with_idx)\r\n",
    "\r\n",
    "    start_time = time.time()\r\n",
    "    trials_inside_circle =  parrallel_obj(async_function(trials_per_worker, i) for i in range(num_workers))\r\n",
    "    pi_estimate = (sum(trials_inside_circle)*4)/float(total_trials)\r\n",
    "\r\n",
    "    print(pi_estimate)\r\n",
    "    print(f\"Time consumed: {time.time()-start_time}\")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting Multiprocessing/pi_estimate_joblib_cached.py\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we run the above code for the first time output is as follows.\r\n",
    "\r\n",
    "<center><image src=\"./img/18.jpg\" width=\"600\"/></center>\r\n",
    "\r\n",
    "But if we did not change the arguments, then results will come from the cache improving the performance significantly.\r\n",
    "\r\n",
    "<center><image src=\"./img/19.jpg\" width=\"600\"/></center>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('High_Performance_Concepts_in_Python-45__Ubn2': virtualenv)"
  },
  "interpreter": {
   "hash": "7e345707fd4776d029d2a8a839d91a2c00b4b8e78ea36281d84f354477ba90d2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}